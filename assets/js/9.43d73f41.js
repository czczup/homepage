(window.webpackJsonp=window.webpackJsonp||[]).push([[9],{210:function(e,t,r){"use strict";var n=r(74);r.n(n).a},223:function(e,t,r){"use strict";r.r(t);r(210);var n=r(0),a=Object(n.a)({},(function(){var e=this,t=e.$createElement,r=e._self._c||t;return r("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[r("h1",{attrs:{id:"work"}},[e._v("Work")]),e._v(" "),r("p",[e._v("Here are some works of mine ðŸ“š")]),e._v(" "),r("h2",{attrs:{id:"publications"}},[e._v("Publications")]),e._v(" "),r("ProjectCard",{attrs:{image:"/projects/vit_adapter.png"}},[r("p",[r("strong",[e._v("Vision Transformer Adapter for Dense Predictions")])]),e._v(" "),r("p",[r("strong",[e._v("Zhe Chen*")]),e._v(", Yuchen Duan*, Wenhai Wang#, Junjun He, Tong Lu#, Jifeng Dai, Yu Qiao")]),e._v(" "),r("p",[e._v("ICLR spotlight, 2023")]),e._v(" "),r("p",[e._v("Introduction: This work present a simple yet powerful adapter for pure ViT, which can remedy the defects of ViT and achieve comparable performance to vision-specific models in dense prediction tasks.")]),e._v(" "),r("p",[e._v("["),r("a",{attrs:{href:"https://arxiv.org/abs/2205.08534",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),r("OutboundLink")],1),e._v("]\n["),r("a",{attrs:{href:"/bibtex/vit_adapter.txt"}},[e._v("BibTex")]),e._v("]\n["),r("a",{attrs:{href:"https://github.com/czczup/ViT-Adapter",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),r("OutboundLink")],1),e._v("]")])]),e._v(" "),r("ProjectCard",{attrs:{image:"/projects/urst.jpg"}},[r("p",[r("strong",[e._v("Towards Ultra-Resolution Neural Style Transfer via Thumbnail Instance Normalization")])]),e._v(" "),r("p",[r("strong",[e._v("Zhe Chen")]),e._v(", Wenhai Wang#, Enze Xie, Tong Lu#, Ping Luo")]),e._v(" "),r("p",[e._v("AAAI, 2022")]),e._v(" "),r("p",[e._v("Introduction: URST is a versatile framework for ultra-high resolution style transfer under limited memory resources.")]),e._v(" "),r("p",[e._v("["),r("a",{attrs:{href:"https://arxiv.org/abs/2103.11784",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),r("OutboundLink")],1),e._v("]\n["),r("a",{attrs:{href:"/bibtex/urst.txt"}},[e._v("BibTex")]),e._v("]\n["),r("a",{attrs:{href:"https://github.com/czczup/URST",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),r("OutboundLink")],1),e._v("]")])]),e._v(" "),r("ProjectCard",{attrs:{image:"/projects/siamese_ccr.jpg"}},[r("p",[r("strong",[e._v("SiameseCCR: A Novel Method for One-shot and Few-shot Chinese CAPTCHA Recognition using Deep Siamese Network")])]),e._v(" "),r("p",[r("strong",[e._v("Zhe Chen")]),e._v(", Weifeng Ma#, Nanfan Xu, Caoting Ji, Yulai Zhang")]),e._v(" "),r("p",[e._v("IET Image Processing, 2020 (SCI Impact Factor: 2.373)")]),e._v(" "),r("p",[e._v("Introduction: We proposed a Siamese network-based method for one-shot and few-shot Chinese CAPTCHA Recognition.")]),e._v(" "),r("p",[e._v("["),r("a",{attrs:{href:"/pdf/SiameseCCR.pdf"}},[e._v("Paper")]),e._v("]\n["),r("a",{attrs:{href:"/bibtex/siameseccr.txt"}},[e._v("BibTex")]),e._v("]\n["),r("a",{attrs:{href:"https://github.com/czczup/SiameseCCR",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),r("OutboundLink")],1),e._v("]")])]),e._v(" "),r("h2",{attrs:{id:"technical-report"}},[e._v("Technical Report")]),e._v(" "),r("ProjectCard",{attrs:{image:"/projects/internimage.png"}},[r("p",[r("strong",[e._v("InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions")])]),e._v(" "),r("p",[e._v("Wenhai Wang*, Jifeng Dai*, "),r("strong",[e._v("Zhe Chen*")]),e._v(", Zhenhang Huang*, Zhiqi Li*, Xizhou Zhu*, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, Xiaogang Wang, Yu Qiao#")]),e._v(" "),r("p",[e._v("Arxiv, 2022")]),e._v(" "),r("p",[e._v("Introduction: This work presents a new large-scale CNN-based foundation model, termed InternImage.")]),e._v(" "),r("p",[e._v("["),r("a",{attrs:{href:"https://arxiv.org/abs/2211.05778",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),r("OutboundLink")],1),e._v("]\n["),r("a",{attrs:{href:"/bibtex/internimage.txt"}},[e._v("BibTex")]),e._v("]\n["),r("a",{attrs:{href:"https://github.com/OpenGVLab/InternImage",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),r("OutboundLink")],1),e._v("]")])]),e._v(" "),r("ProjectCard",{attrs:{image:"/projects/ego4d.png"}},[r("p",[r("strong",[e._v("InternVideo-Ego4D: A Pack of Champion Solutions to Ego4D Challenges")])]),e._v(" "),r("p",[e._v("Guo Chen*, Sen Xing*, "),r("strong",[e._v("Zhe Chen*")]),e._v(", Yi Wang*, Kunchang Li, Yizhuo Li, Yi Liu, Jiahao Wang, Yin-Dong Zheng, Bingkun Huang, Zhiyu Zhao, Junting Pan, Yifei Huang, Zun Wang, Jiashuo Yu, Yinan He, Hongjie Zhang, Tong Lu, Yali Wang, Limin Wang, Yu Qiao#")]),e._v(" "),r("p",[e._v("Arxiv, 2022")]),e._v(" "),r("p",[e._v("Introduction: This work presents our champion solutions to five tracks at Ego4D challenge.")]),e._v(" "),r("p",[e._v("["),r("a",{attrs:{href:"https://arxiv.org/pdf/2211.09529.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),r("OutboundLink")],1),e._v("]\n["),r("a",{attrs:{href:"/bibtex/ego4d.txt"}},[e._v("BibTex")]),e._v("]\n["),r("a",{attrs:{href:"https://github.com/OpenGVLab/ego4d-eccv2022-solutions",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),r("OutboundLink")],1),e._v("]")])]),e._v(" "),r("ProjectCard",{attrs:{image:"/projects/fast.png"}},[r("p",[r("strong",[e._v("FAST: Faster Arbitrarily-Shaped Text Detector with Minimalist Kernel Representation")])]),e._v(" "),r("p",[r("strong",[e._v("Zhe Chen")]),e._v(", Jiahao Wang, Wenhai Wang, Guo Chen, Enze Xie, Ping Luo, Tong Lu#")]),e._v(" "),r("p",[e._v("Arxiv, 2021")]),e._v(" "),r("p",[e._v("Introduction: We propose an accurate and efficient scene text detection framework, termed FAST (i.e., faster arbitrarily-shaped text detector).")]),e._v(" "),r("p",[e._v("["),r("a",{attrs:{href:"https://arxiv.org/abs/2111.02394",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),r("OutboundLink")],1),e._v("]\n["),r("a",{attrs:{href:"/bibtex/fast.txt"}},[e._v("BibTex")]),e._v("]\n["),r("a",{attrs:{href:"https://github.com/czczup/FAST",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),r("OutboundLink")],1),e._v("]")])]),e._v(" "),r("h2",{attrs:{id:"projects"}},[e._v("Projects")]),e._v(" "),r("ProjectCard",{attrs:{image:"/projects/musesart.jpg"}},[r("p",[r("strong",[e._v("MusesArt: A Fast Style Transfer Application based on the Xiaomi's MACE Framework")])]),e._v(" "),r("p",[e._v("Introduction: MusesArt is an Android application to achieve high-resolution nerual style transfer on mobile devices. It is built on OpenCV and Xiaomi's MACE. You can download the source code and compile it with "),r("a",{attrs:{href:"https://developer.android.google.cn/studio/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Android Studio"),r("OutboundLink")],1),e._v(", or install the package we released.")]),e._v(" "),r("p",[e._v("["),r("a",{attrs:{href:"https://github.com/czczup/MusesArt/releases/download/V1.0/app-release.apk",target:"_blank",rel:"noopener noreferrer"}},[e._v("Apk"),r("OutboundLink")],1),e._v("]\n["),r("a",{attrs:{href:"https://github.com/czczup/MusesArt",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),r("OutboundLink")],1),e._v("]")])]),e._v(" "),r("ProjectCard",{attrs:{image:"/projects/captcha_recognition.jpg"}},[r("p",[r("strong",[e._v("English and Chinese Captcha Recognition via TensorFlow")])]),e._v(" "),r("p",[e._v("Introduction: This is our solution for the Captcha Recognition Task of the 9nd National Undergraduate Service Outsourcing Competition.\nThe target of this challenge is to recognize the captcha images with multi levels of difficulty, including digital captcha, English captcha, and Chinese captcha.")]),e._v(" "),r("p",[e._v("["),r("a",{attrs:{href:"https://github.com/czczup/Captcha-Recognition",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),r("OutboundLink")],1),e._v("]")])])],1)}),[],!1,null,null,null);t.default=a.exports},74:function(e,t,r){}}]);