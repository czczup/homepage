(window.webpackJsonp=window.webpackJsonp||[]).push([[7],{208:function(e,r,n){"use strict";var t=n(72);n.n(t).a},220:function(e,r,n){"use strict";n.r(r);n(208);var t=n(0),a=Object(t.a)({},(function(){var e=this,r=e.$createElement,n=e._self._c||r;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("ProfileSection",{attrs:{frontmatter:e.$page.frontmatter}}),e._v(" "),n("h2",{attrs:{id:"about-me"}},[e._v("About Me")]),e._v(" "),n("p",[e._v("I am a second-year PhD candidate in Department of Computer Science and Technology, Nanjing University (NJU) since 2020, supervised by "),n("a",{attrs:{href:"https://cs.nju.edu.cn/lutong/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Prof. Tong Lu"),n("OutboundLink")],1),e._v(".")]),e._v(" "),n("p",[e._v("My research interests are vision foundation model, vision-language model, and detection & segmentation.")]),e._v(" "),n("h2",{attrs:{id:"news"}},[e._v("News")]),e._v(" "),n("ul",[n("li",[e._v("[2024-02-27] "),n("a",{attrs:{href:"https://github.com/OpenGVLab/InternVL",target:"_blank",rel:"noopener noreferrer"}},[e._v("InternVL"),n("OutboundLink")],1),e._v(" is accepted by CVPR 2024.")]),e._v(" "),n("li",[e._v("[2024-01-16] "),n("a",{attrs:{href:"https://kaichen1998.github.io/projects/geodiffusion/",target:"_blank",rel:"noopener noreferrer"}},[e._v("GeoDiffusion"),n("OutboundLink")],1),e._v(", "),n("a",{attrs:{href:"https://arxiv.org/pdf/2308.01907.pdf?ref=morioh.com&utm_source=morioh.com",target:"_blank",rel:"noopener noreferrer"}},[e._v("All-Seeing"),n("OutboundLink")],1),e._v(", and "),n("a",{attrs:{href:"https://openreview.net/pdf?id=lmM4Ecm4HJ",target:"_blank",rel:"noopener noreferrer"}},[e._v("BoS"),n("OutboundLink")],1),e._v(" (spotlight) are accepted by ICLR 2024.")]),e._v(" "),n("li",[e._v("[2023-10-10] "),n("a",{attrs:{href:"https://arxiv.org/abs/2307.01146",target:"_blank",rel:"noopener noreferrer"}},[e._v("AVSegFormer"),n("OutboundLink")],1),e._v(" is accepted by AAAI 2024.")]),e._v(" "),n("li",[e._v("[2023-10-24] "),n("a",{attrs:{href:"https://arxiv.org/abs/2211.05778",target:"_blank",rel:"noopener noreferrer"}},[e._v("InternImage"),n("OutboundLink")],1),e._v(" is is selected as one of "),n("a",{attrs:{href:"https://www.paperdigest.org/2023/09/most-influential-cvpr-papers-2023-09/",target:"_blank",rel:"noopener noreferrer"}},[e._v("CVPR 2023 Top-10 Influential Papers"),n("OutboundLink")],1),e._v(".")]),e._v(" "),n("li",[e._v("[2023-09-22] "),n("a",{attrs:{href:"https://arxiv.org/abs/2305.11175",target:"_blank",rel:"noopener noreferrer"}},[e._v("VisionLLM"),n("OutboundLink")],1),e._v(" is accepted by NeurIPS 2023.")]),e._v(" "),n("li",[e._v("[2023-07-14] "),n("a",{attrs:{href:"https://arxiv.org/abs/2303.17559",target:"_blank",rel:"noopener noreferrer"}},[e._v("DDP"),n("OutboundLink")],1),e._v(" is accepted by ICCV 2023.")]),e._v(" "),n("li",[e._v("[2023-05-10] We release "),n("a",{attrs:{href:"https://github.com/OpenGVLab/InternChat",target:"_blank",rel:"noopener noreferrer"}},[e._v("InternGPT"),n("OutboundLink")],1),e._v(", which allows you to interact with ChatGPT by clicking, dragging and drawing using a pointing device.")]),e._v(" "),n("li",[e._v("[2023-04-20] "),n("a",{attrs:{href:"https://arxiv.org/abs/2305.11424",target:"_blank",rel:"noopener noreferrer"}},[e._v("GPTrans"),n("OutboundLink")],1),e._v(" is accepted by IJCAI 2023.")]),e._v(" "),n("li",[e._v("[2023-02-28] "),n("a",{attrs:{href:"https://arxiv.org/abs/2211.05778",target:"_blank",rel:"noopener noreferrer"}},[e._v("InternImage"),n("OutboundLink")],1),e._v(" (highlight) is accepted by CVPR 2023.")]),e._v(" "),n("li",[e._v("[2023-01-21] "),n("a",{attrs:{href:"https://arxiv.org/abs/2205.08534",target:"_blank",rel:"noopener noreferrer"}},[e._v("ViT-Adapter"),n("OutboundLink")],1),e._v(" (spotlight) is accepted by ICLR 2023.")]),e._v(" "),n("li",[e._v("[2023-01-17] Our team wins the champion of "),n("a",{attrs:{href:"https://codalab.lisn.upsaclay.fr/competitions/7434#learn_the_details",target:"_blank",rel:"noopener noreferrer"}},[e._v("WSDM Cup 2023 Toloka VQA Challenge"),n("OutboundLink")],1),e._v(".")]),e._v(" "),n("li",[e._v("[2022-11-11] Our InternImage-H created new record of "),n("a",{attrs:{href:"https://paperswithcode.com/sota/object-detection-on-coco",target:"_blank",rel:"noopener noreferrer"}},[e._v("65.4 box AP"),n("OutboundLink")],1),e._v(" on COCO "),n("a",{attrs:{href:"https://codalab.lisn.upsaclay.fr/competitions/7384#results",target:"_blank",rel:"noopener noreferrer"}},[e._v("test-dev"),n("OutboundLink")],1),e._v("!")]),e._v(" "),n("li",[e._v("[2022-09-19] Our team wins the champions in 7 tracks of "),n("a",{attrs:{href:"https://ego4d-data.org/workshops/eccv22/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Ego4D ECCV2022 Challenge"),n("OutboundLink")],1),e._v(".")]),e._v(" "),n("li",[e._v("[2021-12-01] "),n("a",{attrs:{href:"https://arxiv.org/abs/2103.11784",target:"_blank",rel:"noopener noreferrer"}},[e._v("URST"),n("OutboundLink")],1),e._v(" is accepted by AAAI 2022.")]),e._v(" "),n("li",[e._v("[2020-12-21] Our team wins the champion of "),n("a",{attrs:{href:"https://cs.nju.edu.cn/b8/d5/c1654a506069/page.htm",target:"_blank",rel:"noopener noreferrer"}},[e._v("NAIC 2020 Remote Sensing Semantic Segmentation Task (1,000,000 RMB bonus)"),n("OutboundLink")],1),e._v(".")]),e._v(" "),n("li",[e._v("[2020-05-12] "),n("a",{attrs:{href:"https://ietresearch.onlinelibrary.wiley.com/doi/epdf/10.1049/iet-ipr.2019.0618",target:"_blank",rel:"noopener noreferrer"}},[e._v("SiameseCCR"),n("OutboundLink")],1),e._v(" is accepted by IET Image Processing.")])]),e._v(" "),n("h2",{attrs:{id:"education-experiences"}},[e._v("Education & Experiences")]),e._v(" "),n("ul",[n("li",[n("p",[n("strong",[e._v("Nanjing University, Nanjing, China")]),e._v(" "),n("br"),e._v("\nSept 2020 - Present")])]),e._v(" "),n("li",[n("p",[n("strong",[e._v("Zhejiang University of Science and Technology, Zhejiang, China")]),e._v(" "),n("br"),e._v("\nSept 2016 - June 2020")])])]),e._v(" "),n("h2",{attrs:{id:"publications"}},[e._v("Publications")]),e._v(" "),n("p",[n("router-link",{attrs:{to:"/projects/"}},[e._v("→ Full list")])],1),e._v(" "),n("p",[e._v("* Equal Contribution      # Corresponding Author")]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/internvl.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks")])]),e._v(" "),n("p",[n("strong",[e._v("Zhe Chen")]),e._v(", Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, Jifeng Dai#")]),e._v(" "),n("p",[e._v("CVPR, 2024")]),e._v(" "),n("p",[e._v("Introduction: InternVL scales up the ViT to 6B parameters and aligns it with LLM.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/abs/2312.14238",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"/bibtex/internvl.txt"}},[e._v("BibTex")]),e._v("]\n["),n("a",{attrs:{href:"https://github.com/OpenGVLab/InternVL",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/675877376",target:"_blank",rel:"noopener noreferrer"}},[e._v("中文解读"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"/poster/internvl.png"}},[e._v("Poster")]),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/internimage.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions")])]),e._v(" "),n("p",[e._v("Wenhai Wang*, Jifeng Dai*, "),n("strong",[e._v("Zhe Chen*")]),e._v(", Zhenhang Huang*, Zhiqi Li*, Xizhou Zhu*, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, Xiaogang Wang, Yu Qiao#")]),e._v(" "),n("p",[e._v("CVPR highlight, 2023 | "),n("a",{attrs:{href:"https://www.paperdigest.org/2023/09/most-influential-cvpr-papers-2023-09/",target:"_blank",rel:"noopener noreferrer"}},[e._v("CVPR 2023 Top-10 Influential Papers"),n("OutboundLink")],1)]),e._v(" "),n("p",[e._v("Introduction: This work presents a new large-scale CNN-based foundation model, termed InternImage.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/abs/2211.05778",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"/bibtex/internimage.txt"}},[e._v("BibTex")]),e._v("]\n["),n("a",{attrs:{href:"https://github.com/OpenGVLab/InternImage",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"/poster/internimage.png"}},[e._v("Poster")]),e._v("]\n["),n("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/610772005",target:"_blank",rel:"noopener noreferrer"}},[e._v("中文解读"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/vit_adapter.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("Vision Transformer Adapter for Dense Predictions")])]),e._v(" "),n("p",[n("strong",[e._v("Zhe Chen*")]),e._v(", Yuchen Duan*, Wenhai Wang#, Junjun He, Tong Lu#, Jifeng Dai, Yu Qiao")]),e._v(" "),n("p",[e._v("ICLR spotlight, 2023")]),e._v(" "),n("p",[e._v("Introduction: This work present a simple yet powerful adapter for pure ViT, which can remedy the defects of ViT and achieve comparable performance to vision-specific models in dense prediction tasks.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/abs/2205.08534",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"/bibtex/vit_adapter.txt"}},[e._v("BibTex")]),e._v("]\n["),n("a",{attrs:{href:"https://github.com/czczup/ViT-Adapter",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"https://iclr.cc/media/PosterPDFs/ICLR%202023/12048.png?t=1680764158.7068026",target:"_blank",rel:"noopener noreferrer"}},[e._v("Poster"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"https://drive.google.com/file/d/1LotIZIEnZzKhsANjBTZs3qcezk9fbVCV/view?usp=share_link",target:"_blank",rel:"noopener noreferrer"}},[e._v("Slides"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/608272954",target:"_blank",rel:"noopener noreferrer"}},[e._v("中文解读"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/visionllm.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks")])]),e._v(" "),n("p",[e._v("Wenhai Wang*, "),n("strong",[e._v("Zhe Chen*")]),e._v(", Xiaokang Chen*, Jiannan Wu*, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, Jifeng Dai#")]),e._v(" "),n("p",[e._v("NeurIPS, 2023")]),e._v(" "),n("p",[e._v("Introduction: We present an LLM-based framework for vision-centric tasks, termed VisionLLM.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/abs/2305.11175",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"/bibtex/visionllm.txt"}},[e._v("BibTex")]),e._v("]\n["),n("a",{attrs:{href:"https://github.com/OpenGVLab/VisionLLM",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"/poster/visionllm.png"}},[e._v("Poster")]),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/ddp.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("DDP: Diffusion Model for Dense Visual Prediction")])]),e._v(" "),n("p",[e._v("Yuanfeng Ji*, "),n("strong",[e._v("Zhe Chen*")]),e._v(", Enze Xie#, Lanqing Hong, Xihui Liu, Zhaoqiang Liu, Tong Lu, Zhenguo Li, Ping Luo")]),e._v(" "),n("p",[e._v("ICCV, 2023")]),e._v(" "),n("p",[e._v("Introduction: We propose a simple, efficient, yet powerful framework for dense visual predictions based on the conditional diffusion pipeline.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/abs/2303.17559",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"/bibtex/ddp.txt"}},[e._v("BibTex")]),e._v("]\n["),n("a",{attrs:{href:"https://github.com/JiYuanFeng/DDP",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"/poster/ddp.png"}},[e._v("Poster")]),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/avsegformer.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("AVSegFormer: Audio-Visual Segmentation with Transformer")])]),e._v(" "),n("p",[e._v("Shengyi Gao, "),n("strong",[e._v("Zhe Chen")]),e._v(", Guo Chen, Wenhai Wang, Tong Lu#")]),e._v(" "),n("p",[e._v("AAAI, 2024")]),e._v(" "),n("p",[e._v("Introduction: This work presents a new framework for AVS tasks that leverages the transformer architecture.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/abs/2307.01146",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"/bibtex/avsegformer.txt"}},[e._v("BibTex")]),e._v("]\n["),n("a",{attrs:{href:"https://github.com/vvvb-github/AVSegFormer",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"/poster/avsegformer.png"}},[e._v("Poster")]),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/gptrans.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("Graph Propagation Transformer for Graph Representation Learning")])]),e._v(" "),n("p",[n("strong",[e._v("Zhe Chen*")]),e._v(", Hao Tan*, Tao Wang, Tianrun Shen, Tong Lu#, Qiuying Peng, Cheng Cheng, Yue Qi")]),e._v(" "),n("p",[e._v("IJCAI, 2023")]),e._v(" "),n("p",[e._v("Introduction: This work presents a novel transformer architecture for graph representation learning.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/abs/2305.11424",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"/bibtex/gptrans.txt"}},[e._v("BibTex")]),e._v("]\n["),n("a",{attrs:{href:"https://github.com/czczup/gptrans",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/urst.jpg",hideBorder:"true"}},[n("p",[n("strong",[e._v("Towards Ultra-Resolution Neural Style Transfer via Thumbnail Instance Normalization")])]),e._v(" "),n("p",[n("strong",[e._v("Zhe Chen")]),e._v(", Wenhai Wang#, Enze Xie, Tong Lu#, Ping Luo")]),e._v(" "),n("p",[e._v("AAAI, 2022")]),e._v(" "),n("p",[e._v("Introduction: URST is a versatile framework for ultra-high resolution style transfer under limited GPU memory resources.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/abs/2103.11784",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"/bibtex/urst.txt"}},[e._v("BibTex")]),e._v("]\n["),n("a",{attrs:{href:"https://github.com/czczup/URST",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"/poster/urst.png"}},[e._v("Poster")]),e._v("]\n["),n("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/360193926",target:"_blank",rel:"noopener noreferrer"}},[e._v("中文解读"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/siamese_ccr.jpg",hideBorder:"true"}},[n("p",[n("strong",[e._v("SiameseCCR: A Novel Method for One-shot and Few-shot Chinese CAPTCHA Recognition using Deep Siamese Network")])]),e._v(" "),n("p",[n("strong",[e._v("Zhe Chen")]),e._v(", Weifeng Ma#, Nanfan Xu, Caoting Ji, Yulai Zhang")]),e._v(" "),n("p",[e._v("IET Image Processing, 2020 (SCI Impact Factor: 2.373)")]),e._v(" "),n("p",[e._v("Introduction: We proposed a Siamese network-based method for one-shot and few-shot Chinese CAPTCHA Recognition.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"/pdf/SiameseCCR.pdf"}},[e._v("Paper")]),e._v("]\n["),n("a",{attrs:{href:"/bibtex/siameseccr.txt"}},[e._v("BibTex")]),e._v("]\n["),n("a",{attrs:{href:"https://github.com/czczup/SiameseCCR",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("h2",{attrs:{id:"preprints"}},[e._v("Preprints")]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/fast.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("FAST: Faster Arbitrarily-Shaped Text Detector with Minimalist Kernel Representation")])]),e._v(" "),n("p",[n("strong",[e._v("Zhe Chen")]),e._v(", Jiahao Wang, Wenhai Wang, Guo Chen, Enze Xie, Ping Luo, Tong Lu#")]),e._v(" "),n("p",[e._v("Arxiv, 2021")]),e._v(" "),n("p",[e._v("Introduction: We propose an accurate and efficient scene text detection framework, termed FAST (i.e., faster arbitrarily-shaped text detector).")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/abs/2111.02394",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"/bibtex/fast.txt"}},[e._v("BibTex")]),e._v("]\n["),n("a",{attrs:{href:"https://github.com/czczup/FAST",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("h2",{attrs:{id:"technical-reports"}},[e._v("Technical Reports")]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/ichat.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language")])]),e._v(" "),n("p",[e._v("Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Yang Yang, Qingyun Li, Jiashuo Yu, Kunchang Li, "),n("strong",[e._v("Zhe Chen")]),e._v(", Xue Yang, Xizhou Zhu, Yali Wang, Limin Wang, Ping Luo, Jifeng Dai, Yu Qiao")]),e._v(" "),n("p",[e._v("Technical Report, 2023")]),e._v(" "),n("p",[e._v("Introduction: InternChat allows you to interact with ChatGPT by clicking, dragging and drawing using a pointing device.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/abs/2305.05662",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"/bibtex/ichat.txt"}},[e._v("BibTex")]),e._v("]\n["),n("a",{attrs:{href:"https://github.com/OpenGVLab/InternChat",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"https://www.zhihu.com/question/570765297/answer/3021584671",target:"_blank",rel:"noopener noreferrer"}},[e._v("中文解读"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/wsdm2023.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("Champion Solution for the WSDM2023 Toloka VQA Challenge")])]),e._v(" "),n("p",[e._v("Shengyi Gao, "),n("strong",[e._v("Zhe Chen")]),e._v(", Guo Chen, Wenhai Wang, Tong Lu#")]),e._v(" "),n("p",[e._v("Technical Report, 2023")]),e._v(" "),n("p",[e._v("Introduction: In this report, we present our champion solution to the WSDM2023 Toloka Visual Question Answering (VQA) Challenge.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/abs/2301.09045",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"/bibtex/wsdm2023.txt"}},[e._v("BibTex")]),e._v("]\n["),n("a",{attrs:{href:"https://github.com/czczup/ViT-Adapter/tree/main/wsdm2023",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/611056687",target:"_blank",rel:"noopener noreferrer"}},[e._v("中文解读"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/ego4d.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("InternVideo-Ego4D: A Pack of Champion Solutions to Ego4D Challenges")])]),e._v(" "),n("p",[e._v("Guo Chen*, Sen Xing*, "),n("strong",[e._v("Zhe Chen*")]),e._v(", Yi Wang*, Kunchang Li, Yizhuo Li, Yi Liu, Jiahao Wang, Yin-Dong Zheng, Bingkun Huang, Zhiyu Zhao, Junting Pan, Yifei Huang, Zun Wang, Jiashuo Yu, Yinan He, Hongjie Zhang, Tong Lu, Yali Wang, Limin Wang, Yu Qiao#")]),e._v(" "),n("p",[e._v("Technical Report, 2022")]),e._v(" "),n("p",[e._v("Introduction: This work presents our champion solutions to five tracks at Ego4D challenge.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/abs/2211.09529",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"/bibtex/ego4d.txt"}},[e._v("BibTex")]),e._v("]\n["),n("a",{attrs:{href:"https://github.com/OpenGVLab/ego4d-eccv2022-solutions",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),n("OutboundLink")],1),e._v("]\n["),n("a",{attrs:{href:"https://mp.weixin.qq.com/s/KsqxA2rp2_2mu6Q73p79fg",target:"_blank",rel:"noopener noreferrer"}},[e._v("中文解读"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("h2",{attrs:{id:"awards-honors"}},[e._v("Awards & Honors")]),e._v(" "),n("h3",{attrs:{id:"contests"}},[e._v("Contests")]),e._v(" "),n("ul",[n("li",[e._v("Toloka Visual Question Answering Challenge, WSDM Cup 2023, 2023, "),n("a",{attrs:{href:"https://codalab.lisn.upsaclay.fr/competitions/7434#learn_the_details",target:"_blank",rel:"noopener noreferrer"}},[n("strong",[e._v("1st Place")]),n("OutboundLink")],1),e._v(".")]),e._v(" "),n("li",[e._v("The 2nd Ego4D Challenge, ECCV Workshop, 2022, "),n("a",{attrs:{href:"https://ego4d-data.org/workshops/eccv22/",target:"_blank",rel:"noopener noreferrer"}},[n("strong",[e._v("7 Top-1 Rankings")]),n("OutboundLink")],1),e._v(".")]),e._v(" "),n("li",[e._v("The 2nd National Artificial Intelligence Challenge (NAIC), Remote Sensing Semantic Segmentation Track, 2020, "),n("a",{attrs:{href:"https://cs.nju.edu.cn/b8/d5/c1654a506069/page.htm",target:"_blank",rel:"noopener noreferrer"}},[n("strong",[e._v("1st Place")]),n("OutboundLink")],1),e._v(" "),n("strong",[e._v("(1,000,000 RMB Bonus)")]),e._v(".")]),e._v(" "),n("li",[e._v("The 2nd China Gaofen Cup Beautiful Countryside Competition, Remote Sensing Crop Classification Track, 2019, "),n("a",{attrs:{href:"https://yjs.zust.edu.cn/info/1035/2176.htm",target:"_blank",rel:"noopener noreferrer"}},[n("strong",[e._v("3rd Prize")]),n("OutboundLink")],1),e._v(" "),n("strong",[e._v("(5,000 RMB Bonus)")]),e._v(".")]),e._v(" "),n("li",[e._v('The 9th National Undergraduate E-commerce "Innovation, Creativity and Entrepreneurship" Challenge, Zhejiang Division, 2019, '),n("a",{attrs:{href:"https://sem.zust.edu.cn/info/1103/1713.htm",target:"_blank",rel:"noopener noreferrer"}},[n("strong",[e._v("1rd Prize")]),n("OutboundLink")],1),e._v(".")]),e._v(" "),n("li",[e._v("The 9nd National Undergraduate Service Outsourcing Competition, Captcha Recognition Task, 2018, "),n("strong",[e._v("2rd Prize")]),e._v(".")])]),e._v(" "),n("h3",{attrs:{id:"honors"}},[e._v("Honors")]),e._v(" "),n("ul",[n("li",[e._v("Outstanding Graduate of Zhejiang Province")]),e._v(" "),n("li",[e._v("Zhejiang Provincial Government Scholarship")])]),e._v(" "),n("h3",{attrs:{id:"some-of-my-friends"}},[e._v("Some of My Friends")]),e._v(" "),n("ul",[n("li",[n("a",{attrs:{href:"https://chenguo.netlify.app/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Guo Chen"),n("OutboundLink")],1),e._v(", "),n("a",{attrs:{href:"https://zhiqi-li.github.io/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Zhiqi Li"),n("OutboundLink")],1),e._v(", "),n("a",{attrs:{href:"https://jiyuanfeng.github.io/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Yuanfeng Ji"),n("OutboundLink")],1),e._v(", "),n("a",{attrs:{href:"https://adriayang.netlify.app/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Yang Yang"),n("OutboundLink")],1),e._v(", Zhanhao Liang")])])],1)}),[],!1,null,null,null);r.default=a.exports},72:function(e,r,n){}}]);