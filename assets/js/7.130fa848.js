(window.webpackJsonp=window.webpackJsonp||[]).push([[7],{208:function(e,n,r){"use strict";var t=r(72);r.n(t).a},220:function(e,n,r){"use strict";r.r(n);r(208);var t=r(0),a=Object(t.a)({},(function(){var e=this,n=e.$createElement,r=e._self._c||n;return r("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[r("ProfileSection",{attrs:{frontmatter:e.$page.frontmatter}}),e._v(" "),r("h2",{attrs:{id:"about-me"}},[e._v("About Me")]),e._v(" "),r("p",[e._v("I am a PhD candidate in Department of Computer Science and Technology, Nanjing University (NJU) since 2020, supervised by "),r("a",{attrs:{href:"https://cs.nju.edu.cn/lutong/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Prof. Tong Lu"),r("OutboundLink")],1),e._v(".")]),e._v(" "),r("p",[e._v("My research interests are computer vision and deep learning. I did some works about vision transformer backbone, detection & segmentation, and neural style transfer.")]),e._v(" "),r("h2",{attrs:{id:"news"}},[e._v("News")]),e._v(" "),r("ul",[r("li",[e._v("[2023-05-10] We release "),r("a",{attrs:{href:"https://github.com/OpenGVLab/InternChat",target:"_blank",rel:"noopener noreferrer"}},[e._v("InternChat"),r("OutboundLink")],1),e._v(", which allows you to interact with ChatGPT by clicking, dragging and drawing using a pointing device.")]),e._v(" "),r("li",[e._v("[2023-02-28] "),r("a",{attrs:{href:"https://arxiv.org/abs/2211.05778",target:"_blank",rel:"noopener noreferrer"}},[e._v("InternImage"),r("OutboundLink")],1),e._v(" (highlight) is accepted by CVPR 2023.")]),e._v(" "),r("li",[e._v("[2023-01-21] "),r("a",{attrs:{href:"https://arxiv.org/abs/2205.08534",target:"_blank",rel:"noopener noreferrer"}},[e._v("ViT-Adapter"),r("OutboundLink")],1),e._v(" (spotlight) is accepted by ICLR 2023.")]),e._v(" "),r("li",[e._v("[2023-01-17] Our team wins the champion of "),r("a",{attrs:{href:"https://codalab.lisn.upsaclay.fr/competitions/7434#learn_the_details",target:"_blank",rel:"noopener noreferrer"}},[e._v("WSDM Cup 2023 Toloka VQA Challenge"),r("OutboundLink")],1),e._v(".")]),e._v(" "),r("li",[e._v("[2022-11-11] Our InternImage-H created new record of "),r("a",{attrs:{href:"https://paperswithcode.com/sota/object-detection-on-coco",target:"_blank",rel:"noopener noreferrer"}},[e._v("65.4 box AP"),r("OutboundLink")],1),e._v(" on COCO "),r("a",{attrs:{href:"https://codalab.lisn.upsaclay.fr/competitions/7384#results",target:"_blank",rel:"noopener noreferrer"}},[e._v("test-dev"),r("OutboundLink")],1),e._v("!")]),e._v(" "),r("li",[e._v("[2022-09-19] Our team wins the champions in 7 tracks of "),r("a",{attrs:{href:"https://ego4d-data.org/workshops/eccv22/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Ego4D ECCV2022 Challenge"),r("OutboundLink")],1),e._v(".")]),e._v(" "),r("li",[e._v("[2021-12-01] "),r("a",{attrs:{href:"https://arxiv.org/abs/2103.11784",target:"_blank",rel:"noopener noreferrer"}},[e._v("URST"),r("OutboundLink")],1),e._v(" is accepted by AAAI 2022.")]),e._v(" "),r("li",[e._v("[2020-12-21] Our team wins the champion of "),r("a",{attrs:{href:"https://cs.nju.edu.cn/b8/d5/c1654a506069/page.htm",target:"_blank",rel:"noopener noreferrer"}},[e._v("NAIC 2020 Remote Sensing Semantic Segmentation Task (1,000,000 RMB bonus)"),r("OutboundLink")],1),e._v(".")]),e._v(" "),r("li",[e._v("[2020-05-12] "),r("a",{attrs:{href:"https://ietresearch.onlinelibrary.wiley.com/doi/epdf/10.1049/iet-ipr.2019.0618",target:"_blank",rel:"noopener noreferrer"}},[e._v("SiameseCCR"),r("OutboundLink")],1),e._v(" is accepted by IET Image Processing.")])]),e._v(" "),r("h2",{attrs:{id:"education-experiences"}},[e._v("Education & Experiences")]),e._v(" "),r("ul",[r("li",[r("p",[r("strong",[e._v("Nanjing University, Nanjing, China")]),e._v(" "),r("br"),e._v("\nSept 2020 - Present")])]),e._v(" "),r("li",[r("p",[r("strong",[e._v("Zhejiang University of Science and Technology, Zhejiang, China")]),e._v(" "),r("br"),e._v("\nSept 2016 - June 2020")])])]),e._v(" "),r("h2",{attrs:{id:"publications"}},[e._v("Publications")]),e._v(" "),r("p",[r("router-link",{attrs:{to:"/projects/"}},[e._v("→ Full list")])],1),e._v(" "),r("p",[e._v("* Equal Contribution      # Corresponding Author")]),e._v(" "),r("ProjectCard",{attrs:{image:"/projects/internimage.png",hideBorder:"true"}},[r("p",[r("strong",[e._v("InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions")])]),e._v(" "),r("p",[e._v("Wenhai Wang*, Jifeng Dai*, "),r("strong",[e._v("Zhe Chen*")]),e._v(", Zhenhang Huang*, Zhiqi Li*, Xizhou Zhu*, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, Xiaogang Wang, Yu Qiao#")]),e._v(" "),r("p",[e._v("CVPR highlight, 2023")]),e._v(" "),r("p",[e._v("Introduction: This work presents a new large-scale CNN-based foundation model, termed InternImage.")]),e._v(" "),r("p",[e._v("["),r("a",{attrs:{href:"https://arxiv.org/abs/2211.05778",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),r("OutboundLink")],1),e._v("]\n["),r("a",{attrs:{href:"/bibtex/internimage.txt"}},[e._v("BibTex")]),e._v("]\n["),r("a",{attrs:{href:"https://github.com/OpenGVLab/InternImage",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),r("OutboundLink")],1),e._v("]\n["),r("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/610772005",target:"_blank",rel:"noopener noreferrer"}},[e._v("中文解读"),r("OutboundLink")],1),e._v("]")])]),e._v(" "),r("ProjectCard",{attrs:{image:"/projects/vit_adapter.png",hideBorder:"true"}},[r("p",[r("strong",[e._v("Vision Transformer Adapter for Dense Predictions")])]),e._v(" "),r("p",[r("strong",[e._v("Zhe Chen*")]),e._v(", Yuchen Duan*, Wenhai Wang#, Junjun He, Tong Lu#, Jifeng Dai, Yu Qiao")]),e._v(" "),r("p",[e._v("ICLR spotlight, 2023")]),e._v(" "),r("p",[e._v("Introduction: This work present a simple yet powerful adapter for pure ViT, which can remedy the defects of ViT and achieve comparable performance to vision-specific models in dense prediction tasks.")]),e._v(" "),r("p",[e._v("["),r("a",{attrs:{href:"https://arxiv.org/abs/2205.08534",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),r("OutboundLink")],1),e._v("]\n["),r("a",{attrs:{href:"/bibtex/vit_adapter.txt"}},[e._v("BibTex")]),e._v("]\n["),r("a",{attrs:{href:"https://github.com/czczup/ViT-Adapter",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),r("OutboundLink")],1),e._v("]\n["),r("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/608272954",target:"_blank",rel:"noopener noreferrer"}},[e._v("中文解读"),r("OutboundLink")],1),e._v("]")])]),e._v(" "),r("ProjectCard",{attrs:{image:"/projects/urst.jpg",hideBorder:"true"}},[r("p",[r("strong",[e._v("Towards Ultra-Resolution Neural Style Transfer via Thumbnail Instance Normalization")])]),e._v(" "),r("p",[r("strong",[e._v("Zhe Chen")]),e._v(", Wenhai Wang#, Enze Xie, Tong Lu#, Ping Luo")]),e._v(" "),r("p",[e._v("AAAI, 2022")]),e._v(" "),r("p",[e._v("Introduction: URST is a versatile framework for ultra-high resolution style transfer under limited GPU memory resources.")]),e._v(" "),r("p",[e._v("["),r("a",{attrs:{href:"https://arxiv.org/abs/2103.11784",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),r("OutboundLink")],1),e._v("]\n["),r("a",{attrs:{href:"/bibtex/urst.txt"}},[e._v("BibTex")]),e._v("]\n["),r("a",{attrs:{href:"https://github.com/czczup/URST",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),r("OutboundLink")],1),e._v("]\n["),r("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/360193926",target:"_blank",rel:"noopener noreferrer"}},[e._v("中文解读"),r("OutboundLink")],1),e._v("]")])]),e._v(" "),r("ProjectCard",{attrs:{image:"/projects/siamese_ccr.jpg",hideBorder:"true"}},[r("p",[r("strong",[e._v("SiameseCCR: A Novel Method for One-shot and Few-shot Chinese CAPTCHA Recognition using Deep Siamese Network")])]),e._v(" "),r("p",[r("strong",[e._v("Zhe Chen")]),e._v(", Weifeng Ma#, Nanfan Xu, Caoting Ji, Yulai Zhang")]),e._v(" "),r("p",[e._v("IET Image Processing, 2020 (SCI Impact Factor: 2.373)")]),e._v(" "),r("p",[e._v("Introduction: We proposed a Siamese network-based method for one-shot and few-shot Chinese CAPTCHA Recognition.")]),e._v(" "),r("p",[e._v("["),r("a",{attrs:{href:"/pdf/SiameseCCR.pdf"}},[e._v("Paper")]),e._v("]\n["),r("a",{attrs:{href:"/bibtex/siameseccr.txt"}},[e._v("BibTex")]),e._v("]\n["),r("a",{attrs:{href:"https://github.com/czczup/SiameseCCR",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),r("OutboundLink")],1),e._v("]")])]),e._v(" "),r("h2",{attrs:{id:"preprints"}},[e._v("Preprints")]),e._v(" "),r("ProjectCard",{attrs:{image:"/projects/ddp.png",hideBorder:"true"}},[r("p",[r("strong",[e._v("DDP: Diffusion Model for Dense Visual Prediction")])]),e._v(" "),r("p",[e._v("Yuanfeng Ji*, "),r("strong",[e._v("Zhe Chen*")]),e._v(", Enze Xie#, Lanqing Hong, Xihui Liu, Zhaoqiang Liu, Tong Lu, Zhenguo Li, Ping Luo")]),e._v(" "),r("p",[e._v("Arxiv, 2023")]),e._v(" "),r("p",[e._v("Introduction: We propose a simple, efficient, yet powerful framework for dense visual predictions based on the conditional diffusion pipeline.")]),e._v(" "),r("p",[e._v("["),r("a",{attrs:{href:"https://arxiv.org/abs/2303.17559",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),r("OutboundLink")],1),e._v("]\n["),r("a",{attrs:{href:"/bibtex/ddp.txt"}},[e._v("BibTex")]),e._v("]\n["),r("a",{attrs:{href:"https://github.com/JiYuanFeng/DDP",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),r("OutboundLink")],1),e._v("]")])]),e._v(" "),r("ProjectCard",{attrs:{image:"/projects/fast.png",hideBorder:"true"}},[r("p",[r("strong",[e._v("FAST: Faster Arbitrarily-Shaped Text Detector with Minimalist Kernel Representation")])]),e._v(" "),r("p",[r("strong",[e._v("Zhe Chen")]),e._v(", Jiahao Wang, Wenhai Wang, Guo Chen, Enze Xie, Ping Luo, Tong Lu#")]),e._v(" "),r("p",[e._v("Arxiv, 2021")]),e._v(" "),r("p",[e._v("Introduction: We propose an accurate and efficient scene text detection framework, termed FAST (i.e., faster arbitrarily-shaped text detector).")]),e._v(" "),r("p",[e._v("["),r("a",{attrs:{href:"https://arxiv.org/abs/2111.02394",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),r("OutboundLink")],1),e._v("]\n["),r("a",{attrs:{href:"/bibtex/fast.txt"}},[e._v("BibTex")]),e._v("]\n["),r("a",{attrs:{href:"https://github.com/czczup/FAST",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),r("OutboundLink")],1),e._v("]")])]),e._v(" "),r("h2",{attrs:{id:"technical-reports"}},[e._v("Technical Reports")]),e._v(" "),r("ProjectCard",{attrs:{image:"/projects/ichat.png",hideBorder:"true"}},[r("p",[r("strong",[e._v("InternChat: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language")])]),e._v(" "),r("p",[e._v("Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Yang Yang, Qingyun Li, Jiashuo Yu, Kunchang Li, "),r("strong",[e._v("Zhe Chen")]),e._v(", Xue Yang, Xizhou Zhu, Yali Wang, Limin Wang, Ping Luo, Jifeng Dai, Yu Qiao")]),e._v(" "),r("p",[e._v("Technical Report, 2023")]),e._v(" "),r("p",[e._v("Introduction: InternChat allows you to interact with ChatGPT by clicking, dragging and drawing using a pointing device.")]),e._v(" "),r("p",[e._v("["),r("a",{attrs:{href:"https://arxiv.org/abs/2305.05662",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),r("OutboundLink")],1),e._v("]\n["),r("a",{attrs:{href:"/bibtex/ichat.txt"}},[e._v("BibTex")]),e._v("]\n["),r("a",{attrs:{href:"https://github.com/OpenGVLab/InternChat",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),r("OutboundLink")],1),e._v("]\n["),r("a",{attrs:{href:"https://www.zhihu.com/question/570765297/answer/3021584671",target:"_blank",rel:"noopener noreferrer"}},[e._v("中文解读"),r("OutboundLink")],1),e._v("]")])]),e._v(" "),r("ProjectCard",{attrs:{image:"/projects/wsdm2023.png",hideBorder:"true"}},[r("p",[r("strong",[e._v("Champion Solution for the WSDM2023 Toloka VQA Challenge")])]),e._v(" "),r("p",[e._v("Shengyi Gao, "),r("strong",[e._v("Zhe Chen")]),e._v(", Guo Chen, Wenhai Wang, Tong Lu#")]),e._v(" "),r("p",[e._v("Technical Report, 2023")]),e._v(" "),r("p",[e._v("Introduction: In this report, we present our champion solution to the WSDM2023 Toloka Visual Question Answering (VQA) Challenge.")]),e._v(" "),r("p",[e._v("["),r("a",{attrs:{href:"https://arxiv.org/abs/2301.09045",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),r("OutboundLink")],1),e._v("]\n["),r("a",{attrs:{href:"/bibtex/wsdm2023.txt"}},[e._v("BibTex")]),e._v("]\n["),r("a",{attrs:{href:"https://github.com/czczup/ViT-Adapter/tree/main/wsdm2023",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),r("OutboundLink")],1),e._v("]\n["),r("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/611056687",target:"_blank",rel:"noopener noreferrer"}},[e._v("中文解读"),r("OutboundLink")],1),e._v("]")])]),e._v(" "),r("ProjectCard",{attrs:{image:"/projects/ego4d.png",hideBorder:"true"}},[r("p",[r("strong",[e._v("InternVideo-Ego4D: A Pack of Champion Solutions to Ego4D Challenges")])]),e._v(" "),r("p",[e._v("Guo Chen*, Sen Xing*, "),r("strong",[e._v("Zhe Chen*")]),e._v(", Yi Wang*, Kunchang Li, Yizhuo Li, Yi Liu, Jiahao Wang, Yin-Dong Zheng, Bingkun Huang, Zhiyu Zhao, Junting Pan, Yifei Huang, Zun Wang, Jiashuo Yu, Yinan He, Hongjie Zhang, Tong Lu, Yali Wang, Limin Wang, Yu Qiao#")]),e._v(" "),r("p",[e._v("Technical Report, 2022")]),e._v(" "),r("p",[e._v("Introduction: This work presents our champion solutions to five tracks at Ego4D challenge.")]),e._v(" "),r("p",[e._v("["),r("a",{attrs:{href:"https://arxiv.org/abs/2211.09529",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),r("OutboundLink")],1),e._v("]\n["),r("a",{attrs:{href:"/bibtex/ego4d.txt"}},[e._v("BibTex")]),e._v("]\n["),r("a",{attrs:{href:"https://github.com/OpenGVLab/ego4d-eccv2022-solutions",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),r("OutboundLink")],1),e._v("]\n["),r("a",{attrs:{href:"https://mp.weixin.qq.com/s/KsqxA2rp2_2mu6Q73p79fg",target:"_blank",rel:"noopener noreferrer"}},[e._v("中文解读"),r("OutboundLink")],1),e._v("]")])]),e._v(" "),r("h2",{attrs:{id:"awards-honors"}},[e._v("Awards & Honors")]),e._v(" "),r("h3",{attrs:{id:"contests"}},[e._v("Contests")]),e._v(" "),r("ul",[r("li",[e._v("Toloka Visual Question Answering Challenge, WSDM Cup 2023, 2023, "),r("a",{attrs:{href:"https://codalab.lisn.upsaclay.fr/competitions/7434#learn_the_details",target:"_blank",rel:"noopener noreferrer"}},[r("strong",[e._v("1st Place")]),r("OutboundLink")],1),e._v(".")]),e._v(" "),r("li",[e._v("The 2nd Ego4D Challenge, ECCV Workshop, 2022, "),r("a",{attrs:{href:"https://ego4d-data.org/workshops/eccv22/",target:"_blank",rel:"noopener noreferrer"}},[r("strong",[e._v("7 Top-1 Rankings")]),r("OutboundLink")],1),e._v(".")]),e._v(" "),r("li",[e._v("The 2nd National Artificial Intelligence Challenge (NAIC), Remote Sensing Semantic Segmentation Task, 2020, "),r("a",{attrs:{href:"https://cs.nju.edu.cn/b8/d5/c1654a506069/page.htm",target:"_blank",rel:"noopener noreferrer"}},[r("strong",[e._v("1st Place")]),r("OutboundLink")],1),e._v(" "),r("strong",[e._v("(1,000,000 RMB Bonus)")]),e._v(".")]),e._v(" "),r("li",[e._v("The 2nd China Gaofen Cup Beautiful Countryside Competition, Remote Sensing Crop Classification Task, 2019, "),r("a",{attrs:{href:"https://yjs.zust.edu.cn/info/1035/2176.htm",target:"_blank",rel:"noopener noreferrer"}},[r("strong",[e._v("3rd Prize")]),r("OutboundLink")],1),e._v(" "),r("strong",[e._v("(5,000 RMB Bonus)")]),e._v(".")]),e._v(" "),r("li",[e._v('The 9th National Undergraduate E-commerce "Innovation, Creativity and Entrepreneurship" Challenge, Zhejiang Division, 2019, '),r("a",{attrs:{href:"https://sem.zust.edu.cn/info/1103/1713.htm",target:"_blank",rel:"noopener noreferrer"}},[r("strong",[e._v("1rd Prize")]),r("OutboundLink")],1),e._v(".")]),e._v(" "),r("li",[e._v("The 9nd National Undergraduate Service Outsourcing Competition, Captcha Recognition Task, 2018, "),r("strong",[e._v("2rd Prize")]),e._v(".")])]),e._v(" "),r("h3",{attrs:{id:"honors"}},[e._v("Honors")]),e._v(" "),r("ul",[r("li",[e._v("Outstanding Graduate of Zhejiang Province")]),e._v(" "),r("li",[e._v("Zhejiang Provincial Government Scholarship")])])],1)}),[],!1,null,null,null);n.default=a.exports},72:function(e,n,r){}}]);